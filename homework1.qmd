---
title: "Homerwork 1"
author: "Jamie McGraw"
date: 2023-05-14
format: 
  docx: default
  html:
    toc: true
    toc_float: true
    code-fold: true
editor: visual
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false



```

# Data Manipulation

## Problem 1: Use logical operators to find flights that:

```         
-   Had an arrival delay of two or more hours (\> 120 minutes)
-   Flew to Houston (IAH or HOU)
-   Were operated by United (`UA`), American (`AA`), or Delta (`DL`)
-   Departed in summer (July, August, and September)
-   Arrived more than two hours late, but didn't leave late
-   Were delayed by at least an hour, but made up over 30 minutes in flight
```

```{r}
#| label: problem-1

# Had an arrival delay of two or more hours (> 120 minutes)
flights %>% 
  filter(arr_delay >= 120) 

# Flew to Houston (IAH or HOU)
flights %>% 
  filter(dest %in% c("IAH","HOU"))

# Were operated by United (`UA`), American (`AA`), or Delta (`DL`)
flights %>% 
  group_by(carrier %in% c("UA","AA","DL"))

# Departed in summer (July, August, and September)
flights %>% 
  filter(month %in% c(7,8,9))
  
# Arrived more than two hours late, but didn't leave late

flights %>% 
  filter(arr_delay>120) %>% 
  filter(dep_delay<0)
# Were delayed by at least an hour, but made up over 30 minutes in flight
  
flights %>% 
  filter(dep_delay>=60) %>%
  filter((dep_delay - arr_delay)>30) %>% 
  select(dep_delay,arr_delay)
```

## Problem 2: What months had the highest and lowest proportion of cancelled flights? Interpret any seasonal patterns. To determine if a flight was cancelled use the following code

<!-- -->

```         
```

```{r}
#| label: problem-2

# What months had the highest and lowest % of cancelled flights?

#interpreted question to mean: which month, on average, had the highest/lowest % of cancelled flights.
#create df for flight calculation and calculate flights missed by month
flights_calcs <- flights %>% 
  filter(is.na(dep_time)) %>%  
  group_by(month) %>% 
  summarise(missed_count = n())
#create df and count the number of flights for each month
total_flights <- flights %>% 
  group_by(month) %>% 
  summarise(flight_count = n())
#insert total flights into first df
flights_calcs$flight_count <- total_flights$flight_count
#create a percentage and make it into a percent and more readable   
flights_calcs$percent_missed <- round((flights_calcs$missed_count/flights_calcs$flight_count)*100,2) 
#arrange df from high to low
flights_calcs %>% 
  arrange(desc(percent_missed))

#interpret question to mean: which single month in the time period had the highest and lowest % of cancelled flights? 
b_flights_calcs <- flights %>% 
  filter(is.na(dep_time)) %>%  
  group_by(year, month) %>% 
  summarise(missed_count = n())
#count the number of flights for each month
b_total_flights <- flights %>% 
  group_by(year,month) %>% 
  summarise(flight_count = n())

b_flights_calcs$flight_count <- total_flights$flight_count
   
b_flights_calcs$percent_missed <- round((flights_calcs$missed_count/flights_calcs$flight_count)*100,2) 

b_flights_calcs %>% 
  arrange(desc(percent_missed))
```

## Problem 3: What plane (specified by the `tailnum` variable) traveled the most times from New York( City airports in 2013? Please `left_join()` the resulting table with the table `planes` (also included in the `nycflights13` package).

For the plane with the greatest number of flights and that had more than 50 seats, please create a table where it flew to during 2013.

```{r}

dplyr::glimpse(flights)
skimr::skim(flights)


planes %>% 
  left_join(
    flights %>% 
      filter(year == 2013, origin %in% c("JFK", "LGA", "EWR"),!is.na(dep_time)) %>%
      count(tailnum), 
      by = 'tailnum') %>% 
    arrange(desc(n))



```

## Problem 4: The `nycflights13` package includes a table (`weather`) that describes the weather during 2013. Use that table to answer the following questions:

```         
-   What is the distribution of temperature (`temp`) in July 2013? Identify any important outliers in terms of the `wind_speed` variable.
-   What is the relationship between `dewp` and `humid`?
-   What is the relationship between `precip` and `visib`?
```

```{r}

dplyr::glimpse(weather)
skimr::skim(weather)
#plot temp, 
hist(weather$temp[weather$year == 2013 & weather$month == 7], main = "July 2013 temperature", xlab= "temp (f)")

#check for outliers, looks like anything above 20 is unusual for that month
hist(weather$wind_speed[weather$year == 2013 & weather$month == 7], breaks = 5, xlim = c(0,50),xlab = "wind speed (mph)")

#relationship between dewp and humid, positive relationship
ggplot(data = weather, aes(x = dewp, y = humid)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

#relationship between precip and visib, negative relationship as we might expect (more visibility given less rain)
ggplot(data = weather, aes(x = visib, y = precip)) + 
  geom_point() +
  geom_smooth(method = "lm", se= TRUE)

```

## Problem 5: Use the `flights` and `planes` tables to answer the following questions:

```         
-   How many planes have a missing date of manufacture?
-   What are the five most common manufacturers?
-   Has the distribution of manufacturer changed over time as reflected by the airplanes flying from NYC in 2013? (Hint: you may need to use case_when() to recode the manufacturer name and collapse rare vendors into a category called Other.)
```

```{r}
install.packages("psych")
library(psych)
dplyr::glimpse(flights)
dplyr::glimpse(planes)

#count number of planes with missing year
count(
  planes %>% 
    filter(is.na(year))
)

#most common manufacturers, boeing and aribus, clear leaders as we'd expect

planes %>% 
  group_by(manufacturer) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))

# Recode manufacturer names
planes_recode <- planes %>%
  mutate(manufacturer = case_when(
    manufacturer %in% c("BOEING", "AIRBUS", "EMBRAER") ~ manufacturer,
    TRUE ~ "Other"
  ))

# Group by year and manufacturer, calculate proportions
planes_dist <- planes_recode %>%
    count(year, manufacturer) %>%
    group_by(year) %>% 
    mutate(prop = n / sum(n)) %>% 
    arrange(year, desc(manufacturer))

#quick stack plot to check the data, seem like there are maybe some errors but we can broadly see Boeing and airbus concentration
ggplot(planes_dist, aes(x = year, y = prop, fill = manufacturer)) +
  geom_area(position = "stack") +
  xlim(1985,2013)+
  labs(title = "change in Manufacturer Distribution Over Time",
    x = "Year",
    y = "Propotion of Planes",
    color = "Manufacturer") +
  theme_minimal()
    
```

\

## Problem 6: Use the `flights` and `planes` tables to answer the following questions:

```         
-   What is the oldest plane (specified by the tailnum variable) that flew from New York City airports in 2013?
-   How many airplanes that flew from New York City are included in the planes table?
```

```{r}

dplyr::glimpse(flights)
dplyr::glimpse(planes)

#get all the tailnumbers for planes that flew in 2013
tailnum_2013 <- flights %>% 
  filter(year == 2013, origin %in% c("JFK", "LGA", "EWR")) %>%  
  group_by(tailnum) %>% 
  summarise(count_flight_2013 = n()) 
  #join this with the year they were made in planes table by tailnum. Oldest model   is 1956
  check_age <- left_join(tailnum_2013,planes,by = 'tailnum') %>% 
    arrange(year)
  #using semi-join to check the numbers, looks like all the planes in the planes table flew from New York but planes doesn't have all the planes from the flights database.
  
  count_planes <- tailnum_2013 %>% 
    semi_join(planes,by = 'tailnum')
  #double checking that all the tailnum in our check above are not duplicated ->  Yes we have 3322 rows and 3322 unique values. 
  n_distinct(count_planes$tailnum)
```

## Problem 7: Use the `nycflights13` to answer the following questions:

```         
-   What is the median arrival delay on a month-by-month basis in each airport?
-   For each airline, plot the median arrival delay for each month and origin airport.
```

```{r}

dplyr::glimpse(flights)
#filter for each year, for each month, for each airport using group by and summarise

med_arr_delays <- flights %>% 
  group_by(year,month,origin) %>% 
  summarise(median_value = median(arr_delay, na.rm = TRUE))
  

#plot the resulting data in ggplot2, start by combining the year and month columns
med_arr_delays$Date = as.Date(paste(med_arr_delays$year,med_arr_delays$month,"01", sep = "-"), format = "%Y-%m-%d")

ggplot(med_arr_delays, aes(x = Date, y = median_value, color = origin)) + 
  geom_line() +
  labs(x = "Date", y = "Median Value") + 
  theme_minimal()
```

## Problem 8: Let's take a closer look at what carriers service the route to San Francisco International (SFO). Join the `flights` and `airlines` tables and count which airlines flew the most to SFO. Produce a new dataframe, `fly_into_sfo` that contains three variables: the `name` of the airline, e.g., `United Air Lines Inc.` not `UA`, the count (number) of times it flew to SFO, and the `percent` of the trips that that particular airline flew to SFO.

```{r}
# Load necessary libraries
library(nycflights13)
library(tidyverse)

# Join flights and airlines, and filter for SFO
flights_to_sfo <- flights %>%
  inner_join(airlines, by = "carrier") %>%
  filter(dest == "SFO")

# Count flights per airline and calculate percent of total
fly_into_sfo <- flights_to_sfo %>%
  group_by(name) %>%
  summarise(count = n(), .groups = "drop") %>%
  mutate(percent = count / sum(count) * 100)

# Print the result
print(fly_into_sfo)



```

And here is some bonus ggplot code to plot your dataframe

```{r}
#| label: ggplot-flights-toSFO
#| message: false
#| warning: false

fly_into_sfo %>% 
  
  # sort 'name' of airline by the numbers it times to flew to SFO
  mutate(name = fct_reorder(name, count)) %>% 
  
  ggplot() +
  
  aes(x = count, 
      y = name) +
  
  # a simple bar/column plot
  geom_col() +
  
  # add labels, so each bar shows the % of total flights 
  geom_text(aes(label = percent),
             hjust = 1, 
             colour = "white", 
             size = 5)+
  
  # add labels to help our audience  
  labs(title="Which airline dominates the NYC to SFO route?", 
       subtitle = "as % of total flights in 2013",
       x= "Number of flights",
       y= NULL) +
  
  theme_minimal() + 
  
  # change the theme-- i just googled those , but you can use the ggThemeAssist add-in
  # https://cran.r-project.org/web/packages/ggThemeAssist/index.html
  
  theme(#
    # so title is left-aligned
    plot.title.position = "plot",
    
    # text in axes appears larger        
    axis.text = element_text(size=12),
    
    # title text is bigger
    plot.title = element_text(size=18)
      ) +

  # add one final layer of NULL, so if you comment out any lines
  # you never end up with a hanging `+` that awaits another ggplot layer
  NULL
 
 
```

## Problem 9: Let's take a look at cancellations of flights to SFO. We create a new dataframe `cancellations` as follows

```{r}

cancellations <- flights %>% 
  
  # just filter for destination == 'SFO'
  filter(dest == 'SFO') %>% 
  
  # a cancelled flight is one with no `dep_time` 
  filter(is.na(dep_time))

print(cancellations)
```

I want you to think how we would organise our data manipulation to create the following plot. No need to write the code, just explain in words how you would go about it.

Start by filtering the data table to include only the flights that were cancelled and originated from either EWR or JFK airports.

Next, group the filtered data based on the month and carrier of the flights.

To summarize the data, count the number of cancelled flights within each group.

After summarizing the data, do a left join operation with the "airline" table to replace the carrier short codes with the corresponding carrier names.

Then create a facet wrap plot that displays two characteristics: the carrier and the name. The plot should use facet_grid() with the "carrier" variable displayed on the vertical axis and the "name" variable displayed on the horizontal axis. The scales on both axes should be set to "free".

To enhance the plot, use geom_text() to add labels to the bars, providing additional information or context for the data represented by each bar\

![](images/sfo-cancellations.png)

## Problem 10: On your own -- Hollywood Age Gap
# Import the dataset and add a column for half_plus_seven_rule
age_gaps <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-02-14/age_gaps.csv', show_col_types = FALSE) %>%
  mutate(half_plus_seven = actor_2_age > ((actor_1_age / 2) + 7) & actor_2_age < ((actor_1_age - 7) * 2))

# Summary statistics and visualization for age_difference
summary_stats <- age_gaps %>%
  summarise(
    min_age_diff = min(age_difference),
    max_age_diff = max(age_difference),
    median_age_diff = median(age_difference),
    mean_age_diff = mean(age_difference),
    sd_age_diff = sd(age_difference)
  )

age_diff_plot <- ggplot(age_gaps, aes(x = age_difference)) +
  geom_histogram(binwidth = 2, fill = "lightblue", color = "white") +
  geom_vline(aes(xintercept = summary_stats$mean_age_diff), color = "darkred", linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = summary_stats$median_age_diff), color = "darkgreen", linetype = "dashed", size = 1) +
  scale_x_continuous(breaks = seq(min(age_gaps$age_difference), max(age_gaps$age_difference), by = 5)) +
  labs(title = "Distribution of Age Difference in Love Interests", x = "Age Difference", y = "Count") +
  theme_minimal()

# Frequency of the half plus seven rule application in the dataset
half_plus_seven_freq <- age_gaps %>%
  group_by(half_plus_seven) %>%
  summarize(frequency = n(), percentage = frequency / nrow(age_gaps) * 100)

# Movie with the greatest number of love interests
movie_count_love_interests <- age_gaps %>%
  group_by(movie_name) %>%
  summarize(love_interests = n_distinct(couple_number)) %>%
  arrange(desc(love_interests))

# Actors/actresses with the greatest number of love interests in the dataset
actor_pairs <- age_gaps %>%
  group_by(group_col1 = pmin(actor_1_name, actor_2_name), group_col2 = pmax(actor_1_name, actor_2_name)) %>%
  summarize(count = n()) %>% 
  arrange(desc(count))

combined_actor_col <- c(actor_pairs$group_col1, actor_pairs$group_col2)
combined_actor_occurrences <- sort(table(combined_actor_col), decreasing = TRUE)

# Mean/median age difference trends over the years (1935 - 2022)
age_diff_over_years <- age_gaps %>%
  group_by(release_year) %>%
  summarise(mean_age_diff = mean(age_difference), median_age_diff = median(age_difference)) %>%
  filter(release_year >= 1935 & release_year <= 2022) %>%
  ggplot(aes(x = release_year)) +
  geom_line(aes(y = mean_age_diff, color = "Mean Age Difference")) +
  geom_line(aes(y = median_age_diff, color = "Median Age Difference")) +
  labs(title = "Mean and Median Age Difference over Years", x = "Release Year", y = "Age Difference", color = "Mean/Median") +
  scale_color_manual(values = c("Mean Age Difference" = "navy", "Median Age Difference"

# Percentage of movies with same-gender love interests by year
percentage_table <- age_gaps %>%
  group_by(release_year) %>%
  summarize(percentage = mean(same_gender) * 100)

scale_x_yr <- seq(min(percentage_table$release_year), max(percentage_table$release_year), by = 3)

same_gender_love_interests_plot <- ggplot(percentage_table, aes(x = release_year, y = percentage)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Release Year", y = "Percentage of Films with Same Gender Love Interests") +
  ggtitle("Percentage of Movies with Same Gender Love Interests by Year") +
  scale_x_continuous(breaks = scale_x_yr, labels = substring(scale_x_yr, nchar(scale_x_yr) - 1)) +
  theme_minimal()


# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Render the edited and completed Quarto Markdown (qmd) file as a Word document (use the "Render" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing tour changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: TYPE NAMES HERE
-   Approximately how much time did you spend on this problem set: ANSWER HERE
-   What, if anything, gave you the most trouble: ANSWER HERE

**Please seek out help when you need it,** and remember the [15-minute rule](https://mam2022.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
